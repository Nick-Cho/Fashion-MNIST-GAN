{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTBLxEIhRgvi",
        "outputId": "e5a76845-f7ec-442e-9d69-c39db635b80f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 100)\n",
            "epoch: 1/30000, d_loss: 0.72,       d_acc: 0.36, g_loss: 0.69\n",
            "epoch: 101/30000, d_loss: 0.00,       d_acc: 1.00, g_loss: 0.00\n",
            "epoch: 201/30000, d_loss: 0.04,       d_acc: 1.00, g_loss: 0.00\n",
            "epoch: 301/30000, d_loss: 0.80,       d_acc: 0.47, g_loss: 0.92\n",
            "epoch: 401/30000, d_loss: 0.75,       d_acc: 0.34, g_loss: 0.65\n",
            "epoch: 501/30000, d_loss: 0.71,       d_acc: 0.45, g_loss: 0.72\n",
            "epoch: 601/30000, d_loss: 0.70,       d_acc: 0.50, g_loss: 0.72\n",
            "epoch: 701/30000, d_loss: 0.66,       d_acc: 0.66, g_loss: 0.74\n",
            "epoch: 801/30000, d_loss: 0.70,       d_acc: 0.55, g_loss: 0.70\n",
            "epoch: 901/30000, d_loss: 0.67,       d_acc: 0.58, g_loss: 0.73\n",
            "epoch: 1001/30000, d_loss: 0.69,       d_acc: 0.55, g_loss: 0.74\n",
            "epoch: 1101/30000, d_loss: 0.69,       d_acc: 0.55, g_loss: 0.75\n",
            "epoch: 1201/30000, d_loss: 0.66,       d_acc: 0.62, g_loss: 0.79\n",
            "epoch: 1301/30000, d_loss: 0.64,       d_acc: 0.75, g_loss: 0.79\n",
            "epoch: 1401/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.74\n",
            "epoch: 1501/30000, d_loss: 0.68,       d_acc: 0.59, g_loss: 0.75\n",
            "epoch: 1601/30000, d_loss: 0.69,       d_acc: 0.47, g_loss: 0.72\n",
            "epoch: 1701/30000, d_loss: 0.69,       d_acc: 0.58, g_loss: 0.72\n",
            "epoch: 1801/30000, d_loss: 0.70,       d_acc: 0.50, g_loss: 0.72\n",
            "epoch: 1901/30000, d_loss: 0.70,       d_acc: 0.53, g_loss: 0.74\n",
            "epoch: 2001/30000, d_loss: 0.69,       d_acc: 0.56, g_loss: 0.71\n",
            "epoch: 2101/30000, d_loss: 0.70,       d_acc: 0.48, g_loss: 0.75\n",
            "epoch: 2201/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.74\n",
            "epoch: 2301/30000, d_loss: 0.69,       d_acc: 0.58, g_loss: 0.73\n",
            "epoch: 2401/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.71\n",
            "epoch: 2501/30000, d_loss: 0.70,       d_acc: 0.50, g_loss: 0.72\n",
            "epoch: 2601/30000, d_loss: 0.70,       d_acc: 0.55, g_loss: 0.74\n",
            "epoch: 2701/30000, d_loss: 0.71,       d_acc: 0.44, g_loss: 0.73\n",
            "epoch: 2801/30000, d_loss: 0.72,       d_acc: 0.41, g_loss: 0.69\n",
            "epoch: 2901/30000, d_loss: 0.73,       d_acc: 0.30, g_loss: 0.69\n",
            "epoch: 3001/30000, d_loss: 0.69,       d_acc: 0.48, g_loss: 0.71\n",
            "epoch: 3101/30000, d_loss: 0.70,       d_acc: 0.39, g_loss: 0.71\n",
            "epoch: 3201/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.72\n",
            "epoch: 3301/30000, d_loss: 0.69,       d_acc: 0.56, g_loss: 0.75\n",
            "epoch: 3401/30000, d_loss: 0.69,       d_acc: 0.59, g_loss: 0.72\n",
            "epoch: 3501/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.70\n",
            "epoch: 3601/30000, d_loss: 0.69,       d_acc: 0.58, g_loss: 0.71\n",
            "epoch: 3701/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.70\n",
            "epoch: 3801/30000, d_loss: 0.69,       d_acc: 0.56, g_loss: 0.69\n",
            "epoch: 3901/30000, d_loss: 0.71,       d_acc: 0.47, g_loss: 0.69\n",
            "epoch: 4001/30000, d_loss: 0.70,       d_acc: 0.52, g_loss: 0.71\n",
            "epoch: 4101/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.72\n",
            "epoch: 4201/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.69\n",
            "epoch: 4301/30000, d_loss: 0.69,       d_acc: 0.48, g_loss: 0.70\n",
            "epoch: 4401/30000, d_loss: 0.69,       d_acc: 0.48, g_loss: 0.73\n",
            "epoch: 4501/30000, d_loss: 0.69,       d_acc: 0.58, g_loss: 0.70\n",
            "epoch: 4601/30000, d_loss: 0.71,       d_acc: 0.44, g_loss: 0.68\n",
            "epoch: 4701/30000, d_loss: 0.67,       d_acc: 0.62, g_loss: 0.75\n",
            "epoch: 4801/30000, d_loss: 0.70,       d_acc: 0.45, g_loss: 0.70\n",
            "epoch: 4901/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.70\n",
            "epoch: 5001/30000, d_loss: 0.70,       d_acc: 0.58, g_loss: 0.71\n",
            "epoch: 5101/30000, d_loss: 0.70,       d_acc: 0.55, g_loss: 0.70\n",
            "epoch: 5201/30000, d_loss: 0.69,       d_acc: 0.48, g_loss: 0.70\n",
            "epoch: 5301/30000, d_loss: 0.71,       d_acc: 0.36, g_loss: 0.69\n",
            "epoch: 5401/30000, d_loss: 0.70,       d_acc: 0.53, g_loss: 0.72\n",
            "epoch: 5501/30000, d_loss: 0.69,       d_acc: 0.55, g_loss: 0.71\n",
            "epoch: 5601/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.68\n",
            "epoch: 5701/30000, d_loss: 0.69,       d_acc: 0.58, g_loss: 0.70\n",
            "epoch: 5801/30000, d_loss: 0.69,       d_acc: 0.59, g_loss: 0.73\n",
            "epoch: 5901/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.72\n",
            "epoch: 6001/30000, d_loss: 0.68,       d_acc: 0.58, g_loss: 0.71\n",
            "epoch: 6101/30000, d_loss: 0.68,       d_acc: 0.55, g_loss: 0.70\n",
            "epoch: 6201/30000, d_loss: 0.69,       d_acc: 0.56, g_loss: 0.70\n",
            "epoch: 6301/30000, d_loss: 0.70,       d_acc: 0.44, g_loss: 0.70\n",
            "epoch: 6401/30000, d_loss: 0.69,       d_acc: 0.55, g_loss: 0.70\n",
            "epoch: 6501/30000, d_loss: 0.69,       d_acc: 0.41, g_loss: 0.71\n",
            "epoch: 6601/30000, d_loss: 0.68,       d_acc: 0.64, g_loss: 0.69\n",
            "epoch: 6701/30000, d_loss: 0.70,       d_acc: 0.42, g_loss: 0.71\n",
            "epoch: 6801/30000, d_loss: 0.69,       d_acc: 0.47, g_loss: 0.69\n",
            "epoch: 6901/30000, d_loss: 0.68,       d_acc: 0.58, g_loss: 0.73\n",
            "epoch: 7001/30000, d_loss: 0.70,       d_acc: 0.61, g_loss: 0.70\n",
            "epoch: 7101/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.70\n",
            "epoch: 7201/30000, d_loss: 0.71,       d_acc: 0.42, g_loss: 0.69\n",
            "epoch: 7301/30000, d_loss: 0.69,       d_acc: 0.55, g_loss: 0.70\n",
            "epoch: 7401/30000, d_loss: 0.68,       d_acc: 0.58, g_loss: 0.71\n",
            "epoch: 7501/30000, d_loss: 0.70,       d_acc: 0.45, g_loss: 0.67\n",
            "epoch: 7601/30000, d_loss: 0.70,       d_acc: 0.53, g_loss: 0.71\n",
            "epoch: 7701/30000, d_loss: 0.70,       d_acc: 0.52, g_loss: 0.72\n",
            "epoch: 7801/30000, d_loss: 0.70,       d_acc: 0.50, g_loss: 0.70\n",
            "epoch: 7901/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.70\n",
            "epoch: 8001/30000, d_loss: 0.70,       d_acc: 0.52, g_loss: 0.70\n",
            "epoch: 8101/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.71\n",
            "epoch: 8201/30000, d_loss: 0.70,       d_acc: 0.52, g_loss: 0.69\n",
            "epoch: 8301/30000, d_loss: 0.70,       d_acc: 0.42, g_loss: 0.70\n",
            "epoch: 8401/30000, d_loss: 0.69,       d_acc: 0.55, g_loss: 0.73\n",
            "epoch: 8501/30000, d_loss: 0.69,       d_acc: 0.56, g_loss: 0.70\n",
            "epoch: 8601/30000, d_loss: 0.70,       d_acc: 0.52, g_loss: 0.69\n",
            "epoch: 8701/30000, d_loss: 0.70,       d_acc: 0.50, g_loss: 0.71\n",
            "epoch: 8801/30000, d_loss: 0.71,       d_acc: 0.36, g_loss: 0.69\n",
            "epoch: 8901/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.72\n",
            "epoch: 9001/30000, d_loss: 0.70,       d_acc: 0.53, g_loss: 0.70\n",
            "epoch: 9101/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.70\n",
            "epoch: 9201/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.69\n",
            "epoch: 9301/30000, d_loss: 0.70,       d_acc: 0.45, g_loss: 0.71\n",
            "epoch: 9401/30000, d_loss: 0.69,       d_acc: 0.58, g_loss: 0.69\n",
            "epoch: 9501/30000, d_loss: 0.70,       d_acc: 0.56, g_loss: 0.70\n",
            "epoch: 9601/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.70\n",
            "epoch: 9701/30000, d_loss: 0.70,       d_acc: 0.50, g_loss: 0.71\n",
            "epoch: 9801/30000, d_loss: 0.70,       d_acc: 0.44, g_loss: 0.70\n",
            "epoch: 9901/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.69\n",
            "epoch: 10001/30000, d_loss: 0.70,       d_acc: 0.50, g_loss: 0.72\n",
            "epoch: 10101/30000, d_loss: 0.69,       d_acc: 0.56, g_loss: 0.69\n",
            "epoch: 10201/30000, d_loss: 0.68,       d_acc: 0.62, g_loss: 0.72\n",
            "epoch: 10301/30000, d_loss: 0.68,       d_acc: 0.64, g_loss: 0.70\n",
            "epoch: 10401/30000, d_loss: 0.69,       d_acc: 0.48, g_loss: 0.73\n",
            "epoch: 10501/30000, d_loss: 0.69,       d_acc: 0.50, g_loss: 0.71\n",
            "epoch: 10601/30000, d_loss: 0.70,       d_acc: 0.44, g_loss: 0.69\n",
            "epoch: 10701/30000, d_loss: 0.70,       d_acc: 0.39, g_loss: 0.69\n",
            "epoch: 10801/30000, d_loss: 0.70,       d_acc: 0.44, g_loss: 0.71\n",
            "epoch: 10901/30000, d_loss: 0.70,       d_acc: 0.45, g_loss: 0.70\n",
            "epoch: 11001/30000, d_loss: 0.69,       d_acc: 0.59, g_loss: 0.70\n",
            "epoch: 11101/30000, d_loss: 0.70,       d_acc: 0.48, g_loss: 0.71\n",
            "epoch: 11201/30000, d_loss: 0.69,       d_acc: 0.58, g_loss: 0.71\n",
            "epoch: 11301/30000, d_loss: 0.70,       d_acc: 0.45, g_loss: 0.70\n",
            "epoch: 11401/30000, d_loss: 0.69,       d_acc: 0.47, g_loss: 0.71\n",
            "epoch: 11501/30000, d_loss: 0.69,       d_acc: 0.50, g_loss: 0.71\n",
            "epoch: 11601/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.71\n",
            "epoch: 11701/30000, d_loss: 0.70,       d_acc: 0.41, g_loss: 0.69\n",
            "epoch: 11801/30000, d_loss: 0.68,       d_acc: 0.62, g_loss: 0.71\n",
            "epoch: 11901/30000, d_loss: 0.70,       d_acc: 0.42, g_loss: 0.70\n",
            "epoch: 12001/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.70\n",
            "epoch: 12101/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.70\n",
            "epoch: 12201/30000, d_loss: 0.70,       d_acc: 0.39, g_loss: 0.70\n",
            "epoch: 12301/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.70\n",
            "epoch: 12401/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.70\n",
            "epoch: 12501/30000, d_loss: 0.68,       d_acc: 0.58, g_loss: 0.72\n",
            "epoch: 12601/30000, d_loss: 0.68,       d_acc: 0.61, g_loss: 0.68\n",
            "epoch: 12701/30000, d_loss: 0.69,       d_acc: 0.47, g_loss: 0.71\n",
            "epoch: 12801/30000, d_loss: 0.70,       d_acc: 0.44, g_loss: 0.71\n",
            "epoch: 12901/30000, d_loss: 0.69,       d_acc: 0.50, g_loss: 0.71\n",
            "epoch: 13001/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.71\n",
            "epoch: 13101/30000, d_loss: 0.69,       d_acc: 0.59, g_loss: 0.69\n",
            "epoch: 13201/30000, d_loss: 0.69,       d_acc: 0.48, g_loss: 0.70\n",
            "epoch: 13301/30000, d_loss: 0.69,       d_acc: 0.50, g_loss: 0.69\n",
            "epoch: 13401/30000, d_loss: 0.68,       d_acc: 0.59, g_loss: 0.69\n",
            "epoch: 13501/30000, d_loss: 0.70,       d_acc: 0.44, g_loss: 0.71\n",
            "epoch: 13601/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.71\n",
            "epoch: 13701/30000, d_loss: 0.70,       d_acc: 0.53, g_loss: 0.70\n",
            "epoch: 13801/30000, d_loss: 0.71,       d_acc: 0.45, g_loss: 0.69\n",
            "epoch: 13901/30000, d_loss: 0.69,       d_acc: 0.45, g_loss: 0.70\n",
            "epoch: 14001/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.70\n",
            "epoch: 14101/30000, d_loss: 0.70,       d_acc: 0.39, g_loss: 0.70\n",
            "epoch: 14201/30000, d_loss: 0.69,       d_acc: 0.55, g_loss: 0.69\n",
            "epoch: 14301/30000, d_loss: 0.70,       d_acc: 0.48, g_loss: 0.67\n",
            "epoch: 14401/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.69\n",
            "epoch: 14501/30000, d_loss: 0.69,       d_acc: 0.62, g_loss: 0.71\n",
            "epoch: 14601/30000, d_loss: 0.68,       d_acc: 0.61, g_loss: 0.67\n",
            "epoch: 14701/30000, d_loss: 0.71,       d_acc: 0.42, g_loss: 0.70\n",
            "epoch: 14801/30000, d_loss: 0.68,       d_acc: 0.58, g_loss: 0.72\n",
            "epoch: 14901/30000, d_loss: 0.70,       d_acc: 0.45, g_loss: 0.70\n",
            "epoch: 15001/30000, d_loss: 0.70,       d_acc: 0.50, g_loss: 0.71\n",
            "epoch: 15101/30000, d_loss: 0.70,       d_acc: 0.52, g_loss: 0.70\n",
            "epoch: 15201/30000, d_loss: 0.70,       d_acc: 0.48, g_loss: 0.69\n",
            "epoch: 15301/30000, d_loss: 0.69,       d_acc: 0.61, g_loss: 0.72\n",
            "epoch: 15401/30000, d_loss: 0.70,       d_acc: 0.50, g_loss: 0.71\n",
            "epoch: 15501/30000, d_loss: 0.72,       d_acc: 0.38, g_loss: 0.71\n",
            "epoch: 15601/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.71\n",
            "epoch: 15701/30000, d_loss: 0.70,       d_acc: 0.50, g_loss: 0.70\n",
            "epoch: 15801/30000, d_loss: 0.70,       d_acc: 0.45, g_loss: 0.70\n",
            "epoch: 15901/30000, d_loss: 0.71,       d_acc: 0.42, g_loss: 0.71\n",
            "epoch: 16001/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.70\n",
            "epoch: 16101/30000, d_loss: 0.69,       d_acc: 0.50, g_loss: 0.69\n",
            "epoch: 16201/30000, d_loss: 0.68,       d_acc: 0.56, g_loss: 0.72\n",
            "epoch: 16301/30000, d_loss: 0.70,       d_acc: 0.48, g_loss: 0.71\n",
            "epoch: 16401/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.68\n",
            "epoch: 16501/30000, d_loss: 0.69,       d_acc: 0.50, g_loss: 0.69\n",
            "epoch: 16601/30000, d_loss: 0.69,       d_acc: 0.55, g_loss: 0.72\n",
            "epoch: 16701/30000, d_loss: 0.70,       d_acc: 0.42, g_loss: 0.72\n",
            "epoch: 16801/30000, d_loss: 0.68,       d_acc: 0.61, g_loss: 0.71\n",
            "epoch: 16901/30000, d_loss: 0.70,       d_acc: 0.53, g_loss: 0.72\n",
            "epoch: 17001/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.72\n",
            "epoch: 17101/30000, d_loss: 0.71,       d_acc: 0.45, g_loss: 0.68\n",
            "epoch: 17201/30000, d_loss: 0.70,       d_acc: 0.53, g_loss: 0.70\n",
            "epoch: 17301/30000, d_loss: 0.70,       d_acc: 0.48, g_loss: 0.71\n",
            "epoch: 17401/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.73\n",
            "epoch: 17501/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.69\n",
            "epoch: 17601/30000, d_loss: 0.70,       d_acc: 0.48, g_loss: 0.70\n",
            "epoch: 17701/30000, d_loss: 0.70,       d_acc: 0.50, g_loss: 0.70\n",
            "epoch: 17801/30000, d_loss: 0.72,       d_acc: 0.45, g_loss: 0.71\n",
            "epoch: 17901/30000, d_loss: 0.68,       d_acc: 0.61, g_loss: 0.70\n",
            "epoch: 18001/30000, d_loss: 0.70,       d_acc: 0.44, g_loss: 0.72\n",
            "epoch: 18101/30000, d_loss: 0.69,       d_acc: 0.55, g_loss: 0.68\n",
            "epoch: 18201/30000, d_loss: 0.70,       d_acc: 0.48, g_loss: 0.71\n",
            "epoch: 18301/30000, d_loss: 0.67,       d_acc: 0.67, g_loss: 0.68\n",
            "epoch: 18401/30000, d_loss: 0.70,       d_acc: 0.50, g_loss: 0.69\n",
            "epoch: 18501/30000, d_loss: 0.68,       d_acc: 0.53, g_loss: 0.72\n",
            "epoch: 18601/30000, d_loss: 0.69,       d_acc: 0.55, g_loss: 0.70\n",
            "epoch: 18701/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.68\n",
            "epoch: 18801/30000, d_loss: 0.70,       d_acc: 0.44, g_loss: 0.71\n",
            "epoch: 18901/30000, d_loss: 0.70,       d_acc: 0.48, g_loss: 0.70\n",
            "epoch: 19001/30000, d_loss: 0.69,       d_acc: 0.48, g_loss: 0.69\n",
            "epoch: 19101/30000, d_loss: 0.68,       d_acc: 0.59, g_loss: 0.70\n",
            "epoch: 19201/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.71\n",
            "epoch: 19301/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.69\n",
            "epoch: 19401/30000, d_loss: 0.69,       d_acc: 0.58, g_loss: 0.69\n",
            "epoch: 19501/30000, d_loss: 0.68,       d_acc: 0.56, g_loss: 0.71\n",
            "epoch: 19601/30000, d_loss: 0.69,       d_acc: 0.47, g_loss: 0.74\n",
            "epoch: 19701/30000, d_loss: 0.69,       d_acc: 0.55, g_loss: 0.71\n",
            "epoch: 19801/30000, d_loss: 0.70,       d_acc: 0.55, g_loss: 0.71\n",
            "epoch: 19901/30000, d_loss: 0.69,       d_acc: 0.48, g_loss: 0.68\n",
            "epoch: 20001/30000, d_loss: 0.71,       d_acc: 0.44, g_loss: 0.71\n",
            "epoch: 20101/30000, d_loss: 0.70,       d_acc: 0.44, g_loss: 0.70\n",
            "epoch: 20201/30000, d_loss: 0.69,       d_acc: 0.56, g_loss: 0.70\n",
            "epoch: 20301/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.70\n",
            "epoch: 20401/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.71\n",
            "epoch: 20501/30000, d_loss: 0.68,       d_acc: 0.58, g_loss: 0.69\n",
            "epoch: 20601/30000, d_loss: 0.70,       d_acc: 0.52, g_loss: 0.70\n",
            "epoch: 20701/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.68\n",
            "epoch: 20801/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.68\n",
            "epoch: 20901/30000, d_loss: 0.68,       d_acc: 0.67, g_loss: 0.71\n",
            "epoch: 21001/30000, d_loss: 0.68,       d_acc: 0.55, g_loss: 0.75\n",
            "epoch: 21101/30000, d_loss: 0.71,       d_acc: 0.44, g_loss: 0.72\n",
            "epoch: 21201/30000, d_loss: 0.69,       d_acc: 0.58, g_loss: 0.68\n",
            "epoch: 21301/30000, d_loss: 0.70,       d_acc: 0.52, g_loss: 0.71\n",
            "epoch: 21401/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.71\n",
            "epoch: 21501/30000, d_loss: 0.69,       d_acc: 0.61, g_loss: 0.71\n",
            "epoch: 21601/30000, d_loss: 0.69,       d_acc: 0.56, g_loss: 0.69\n",
            "epoch: 21701/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.70\n",
            "epoch: 21801/30000, d_loss: 0.69,       d_acc: 0.58, g_loss: 0.69\n",
            "epoch: 21901/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.69\n",
            "epoch: 22001/30000, d_loss: 0.68,       d_acc: 0.56, g_loss: 0.67\n",
            "epoch: 22101/30000, d_loss: 0.70,       d_acc: 0.48, g_loss: 0.69\n",
            "epoch: 22201/30000, d_loss: 0.69,       d_acc: 0.59, g_loss: 0.71\n",
            "epoch: 22301/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.69\n",
            "epoch: 22401/30000, d_loss: 0.70,       d_acc: 0.52, g_loss: 0.72\n",
            "epoch: 22501/30000, d_loss: 0.68,       d_acc: 0.62, g_loss: 0.72\n",
            "epoch: 22601/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.71\n",
            "epoch: 22701/30000, d_loss: 0.69,       d_acc: 0.56, g_loss: 0.72\n",
            "epoch: 22801/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.71\n",
            "epoch: 22901/30000, d_loss: 0.70,       d_acc: 0.42, g_loss: 0.69\n",
            "epoch: 23001/30000, d_loss: 0.70,       d_acc: 0.45, g_loss: 0.70\n",
            "epoch: 23101/30000, d_loss: 0.69,       d_acc: 0.59, g_loss: 0.68\n",
            "epoch: 23201/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.71\n",
            "epoch: 23301/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.71\n",
            "epoch: 23401/30000, d_loss: 0.69,       d_acc: 0.50, g_loss: 0.71\n",
            "epoch: 23501/30000, d_loss: 0.69,       d_acc: 0.45, g_loss: 0.69\n",
            "epoch: 23601/30000, d_loss: 0.67,       d_acc: 0.62, g_loss: 0.70\n",
            "epoch: 23701/30000, d_loss: 0.69,       d_acc: 0.56, g_loss: 0.72\n",
            "epoch: 23801/30000, d_loss: 0.69,       d_acc: 0.48, g_loss: 0.71\n",
            "epoch: 23901/30000, d_loss: 0.68,       d_acc: 0.50, g_loss: 0.71\n",
            "epoch: 24001/30000, d_loss: 0.69,       d_acc: 0.56, g_loss: 0.72\n",
            "epoch: 24101/30000, d_loss: 0.70,       d_acc: 0.50, g_loss: 0.71\n",
            "epoch: 24201/30000, d_loss: 0.70,       d_acc: 0.48, g_loss: 0.71\n",
            "epoch: 24301/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.72\n",
            "epoch: 24401/30000, d_loss: 0.70,       d_acc: 0.44, g_loss: 0.70\n",
            "epoch: 24501/30000, d_loss: 0.69,       d_acc: 0.55, g_loss: 0.70\n",
            "epoch: 24601/30000, d_loss: 0.71,       d_acc: 0.47, g_loss: 0.67\n",
            "epoch: 24701/30000, d_loss: 0.70,       d_acc: 0.45, g_loss: 0.70\n",
            "epoch: 24801/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.71\n",
            "epoch: 24901/30000, d_loss: 0.70,       d_acc: 0.45, g_loss: 0.71\n",
            "epoch: 25001/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.72\n",
            "epoch: 25101/30000, d_loss: 0.72,       d_acc: 0.39, g_loss: 0.71\n",
            "epoch: 25201/30000, d_loss: 0.69,       d_acc: 0.58, g_loss: 0.72\n",
            "epoch: 25301/30000, d_loss: 0.70,       d_acc: 0.42, g_loss: 0.70\n",
            "epoch: 25401/30000, d_loss: 0.70,       d_acc: 0.45, g_loss: 0.70\n",
            "epoch: 25501/30000, d_loss: 0.68,       d_acc: 0.67, g_loss: 0.71\n",
            "epoch: 25601/30000, d_loss: 0.68,       d_acc: 0.59, g_loss: 0.71\n",
            "epoch: 25701/30000, d_loss: 0.68,       d_acc: 0.58, g_loss: 0.72\n",
            "epoch: 25801/30000, d_loss: 0.69,       d_acc: 0.56, g_loss: 0.70\n",
            "epoch: 25901/30000, d_loss: 0.68,       d_acc: 0.62, g_loss: 0.70\n",
            "epoch: 26001/30000, d_loss: 0.69,       d_acc: 0.56, g_loss: 0.70\n",
            "epoch: 26101/30000, d_loss: 0.69,       d_acc: 0.48, g_loss: 0.71\n",
            "epoch: 26201/30000, d_loss: 0.68,       d_acc: 0.56, g_loss: 0.75\n",
            "epoch: 26301/30000, d_loss: 0.71,       d_acc: 0.47, g_loss: 0.71\n",
            "epoch: 26401/30000, d_loss: 0.70,       d_acc: 0.50, g_loss: 0.72\n",
            "epoch: 26501/30000, d_loss: 0.70,       d_acc: 0.52, g_loss: 0.71\n",
            "epoch: 26601/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.68\n",
            "epoch: 26701/30000, d_loss: 0.70,       d_acc: 0.45, g_loss: 0.71\n",
            "epoch: 26801/30000, d_loss: 0.70,       d_acc: 0.48, g_loss: 0.68\n",
            "epoch: 26901/30000, d_loss: 0.70,       d_acc: 0.50, g_loss: 0.70\n",
            "epoch: 27001/30000, d_loss: 0.69,       d_acc: 0.56, g_loss: 0.71\n",
            "epoch: 27101/30000, d_loss: 0.70,       d_acc: 0.45, g_loss: 0.72\n",
            "epoch: 27201/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.70\n",
            "epoch: 27301/30000, d_loss: 0.67,       d_acc: 0.66, g_loss: 0.70\n",
            "epoch: 27401/30000, d_loss: 0.68,       d_acc: 0.61, g_loss: 0.69\n",
            "epoch: 27501/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.69\n",
            "epoch: 27601/30000, d_loss: 0.69,       d_acc: 0.59, g_loss: 0.69\n",
            "epoch: 27701/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.71\n",
            "epoch: 27801/30000, d_loss: 0.71,       d_acc: 0.48, g_loss: 0.66\n",
            "epoch: 27901/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.71\n",
            "epoch: 28001/30000, d_loss: 0.70,       d_acc: 0.53, g_loss: 0.71\n",
            "epoch: 28101/30000, d_loss: 0.69,       d_acc: 0.47, g_loss: 0.71\n",
            "epoch: 28201/30000, d_loss: 0.70,       d_acc: 0.48, g_loss: 0.72\n",
            "epoch: 28301/30000, d_loss: 0.69,       d_acc: 0.59, g_loss: 0.68\n",
            "epoch: 28401/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.71\n",
            "epoch: 28501/30000, d_loss: 0.69,       d_acc: 0.53, g_loss: 0.72\n",
            "epoch: 28601/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.68\n",
            "epoch: 28701/30000, d_loss: 0.68,       d_acc: 0.62, g_loss: 0.71\n",
            "epoch: 28801/30000, d_loss: 0.70,       d_acc: 0.52, g_loss: 0.70\n",
            "epoch: 28901/30000, d_loss: 0.69,       d_acc: 0.50, g_loss: 0.73\n",
            "epoch: 29001/30000, d_loss: 0.72,       d_acc: 0.34, g_loss: 0.70\n",
            "epoch: 29101/30000, d_loss: 0.68,       d_acc: 0.53, g_loss: 0.70\n",
            "epoch: 29201/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.74\n",
            "epoch: 29301/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.69\n",
            "epoch: 29401/30000, d_loss: 0.69,       d_acc: 0.48, g_loss: 0.71\n",
            "epoch: 29501/30000, d_loss: 0.69,       d_acc: 0.52, g_loss: 0.72\n",
            "epoch: 29601/30000, d_loss: 0.69,       d_acc: 0.58, g_loss: 0.75\n",
            "epoch: 29701/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.69\n",
            "epoch: 29801/30000, d_loss: 0.68,       d_acc: 0.62, g_loss: 0.72\n",
            "epoch: 29901/30000, d_loss: 0.70,       d_acc: 0.47, g_loss: 0.69\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout, Reshape, Conv2D, Conv2DTranspose, Flatten, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "\n",
        "import numpy as np\n",
        "# import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Load in data\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalizing inputs\n",
        "x_train, x_test = x_train/255.0*2-1, x_test/255.0*2-1\n",
        "#print(\"x_train.shape\", x_train.shape)\n",
        "\n",
        "# Flattening Data\n",
        "N, H, W = x_train.shape  # N-samples, H-height, W-width\n",
        "D = H*W  # D- number of pixels in the image\n",
        "\n",
        "#print(\"x_train shape: \", x_train.shape)\n",
        "\n",
        "latent_dim = 100  # latent space dimension\n",
        "\n",
        "# Generator model\n",
        "\n",
        "\n",
        "def build_generator(latent_dim):\n",
        "    i = Input(shape=(latent_dim,))\n",
        "    x = Dense(7*7*128, activation=LeakyReLU(alpha=0.2))(i)\n",
        "    x = Reshape([7, 7, 128])(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2DTranspose(64, kernel_size=5, strides=2, padding=\"same\",\n",
        "                        activation=\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2DTranspose(1, kernel_size=5, strides=2, padding=\"same\",\n",
        "                        activation=\"tanh\")(x)\n",
        "    model = Model(i, x)\n",
        "    return model\n",
        "\n",
        "# Discriminator model\n",
        "\n",
        "\n",
        "def build_discriminator(img_size):\n",
        "    i = Input(shape=(img_size),)\n",
        "    x = Conv2D(64, kernel_size=5, strides=2, padding=\"same\",\n",
        "               activation=LeakyReLU(0.3))(i)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Conv2D(64, kernel_size=5, strides=2, padding=\"same\",\n",
        "               activation=LeakyReLU(0.3))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = Model(i, x)\n",
        "    return model\n",
        "\n",
        "\n",
        "# building discriminator model\n",
        "discriminator = build_discriminator([28, 28, 1])\n",
        "discriminator.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=Adam(0.0002, 0.5),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "# building generator model\n",
        "generator = build_generator(latent_dim)\n",
        "\n",
        "# Creating noise sample and passing into generator\n",
        "z = Input(shape=(latent_dim,))\n",
        "print(z.shape)\n",
        "img = generator(z)\n",
        "\n",
        "# Ensure generator is being trained fully before discriminator is trying to decipher image\n",
        "discriminator.trainable = False\n",
        "\n",
        "# 1 means picture is fake\n",
        "fake_pred = discriminator(img)\n",
        "\n",
        "gan = Model(z, fake_pred)\n",
        "\n",
        "# Compile Combined model to adjust weights\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "# Training the GAN\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 30000\n",
        "sample_period = 200  # data is saved every time the sample period passes\n",
        "\n",
        "# Creating Batch Labels\n",
        "ones = np.ones(batch_size)\n",
        "zeros = np.zeros(batch_size)\n",
        "\n",
        "d_losses = []  # discriminator loss\n",
        "g_losses = []  # generator loss\n",
        "\n",
        "# Creating folder to store images\n",
        "if not os.path.exists('gan_images'):\n",
        "    os.makedirs('gan_images')\n",
        "\n",
        "\n",
        "def sample_images(epoch):\n",
        "    rows, cols = 5, 5\n",
        "    # creating noise to input into the generator\n",
        "    noise = np.random.randn(rows*cols, latent_dim)\n",
        "    imgs = generator.predict(noise)\n",
        "\n",
        "    # Rescale images to 0-1\n",
        "    imgs = 0.5 * imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(rows, cols)\n",
        "    idx = 0\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            axs[i, j].imshow(imgs[idx].reshape(H, W), cmap='gray')\n",
        "            axs[i, j].axis('off')\n",
        "            idx += 1\n",
        "    fig.savefig(\"gan_images/%d.png\" % epoch)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training discriminator\n",
        "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "    real_imgs = x_train[idx]\n",
        "  \n",
        "    noise = np.random.randn(batch_size, latent_dim)\n",
        "    fake_imgs = generator.predict(noise)\n",
        "\n",
        "    d_loss_real, d_acc_real = discriminator.train_on_batch(real_imgs, ones)\n",
        "    d_loss_fake, d_acc_fake = discriminator.train_on_batch(fake_imgs, zeros)\n",
        "    d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
        "    d_acc = 0.5 * (d_acc_real + d_acc_fake)\n",
        "\n",
        "    # Training generator\n",
        "    noise = np.random.randn(batch_size, latent_dim)\n",
        "    g_loss = gan.train_on_batch(noise, ones)\n",
        "\n",
        "    noise = np.random.randn(batch_size, latent_dim)\n",
        "    g_loss = gan.train_on_batch(noise, ones)\n",
        "\n",
        "    # Save the losses\n",
        "    d_losses.append(d_loss)\n",
        "    g_losses.append(g_loss)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"epoch: {epoch+1}/{epochs}, d_loss: {d_loss:.2f}, \\\n",
        "      d_acc: {d_acc:.2f}, g_loss: {g_loss:.2f}\")\n",
        "\n",
        "    if epoch % sample_period == 0:\n",
        "        sample_images(epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz-Sok6XX45r",
        "outputId": "5449859e-2a29-4204-bd52-e55a29b0262a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "x_train.shape"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Fashion_MNIST_GAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}